{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Our task is simple, recognize handwritten digits. We will use MNIST dataset for this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary library\n",
    "In this tutorial, we are going to use pytorch, the cutting-edge deep learning framework to complete our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to dataset/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST\\raw\\train-images-idx3-ubyte.gz to dataset/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File not found or corrupted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m image_transform \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     11\u001b[0m                                torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     12\u001b[0m                                torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mNormalize(\n\u001b[0;32m     13\u001b[0m                                  (\u001b[38;5;241m0.1307\u001b[39m,), (\u001b[38;5;241m0.3081\u001b[39m,))\n\u001b[0;32m     14\u001b[0m                              ])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#image datasets\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     21\u001b[0m                                           train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m     22\u001b[0m                                           download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m                                           transform\u001b[38;5;241m=\u001b[39mimage_transform)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#data loaders\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sathvik Malgikar\\.conda\\envs\\distro3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:99\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sathvik Malgikar\\.conda\\envs\\distro3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:187\u001b[0m, in \u001b[0;36mMNIST.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 187\u001b[0m     \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to download (trying next):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sathvik Malgikar\\.conda\\envs\\distro3\\Lib\\site-packages\\torchvision\\datasets\\utils.py:378\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[0;32m    376\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 378\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sathvik Malgikar\\.conda\\envs\\distro3\\Lib\\site-packages\\torchvision\\datasets\\utils.py:151\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# check integrity of downloaded file\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(fpath, md5):\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found or corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File not found or corrupted."
     ]
    }
   ],
   "source": [
    "## Create dataloader, in PyTorch, we feed the trainer data with use of dataloader\n",
    "## We create dataloader with dataset from torchvision, \n",
    "## and we dont have to download it seperately, all automatically done\n",
    "\n",
    "# Define batch size, batch size is how much data you feed for training in one iteration\n",
    "batch_size_train = 64 # We use a small batch size here for training\n",
    "batch_size_test = 1024 #\n",
    "\n",
    "# define how image transformed\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])\n",
    "#image datasets\n",
    "train_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                           train=True, \n",
    "                                           download=True,\n",
    "                                           transform=image_transform)\n",
    "test_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                          train=False, \n",
    "                                          download=True,\n",
    "                                          transform=image_transform)\n",
    "#data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size_test, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADidJREFUeJzt3X+MVfWZx/HPIxSjloQfDZRYVkohGxsS7GZi1rhZ3RjGXzWA+KOSNJglHf+oydYQU0Mk1WyIZkO71gSbDAELoWOpogup1VKNOtUUFSZQpWwpIdhOIQNok1qNQZxn/5gzmynM/Z47955zzx2e9ysh98dzzzlPbvjMOfd+z7lfc3cBiOeCqhsAUA3CDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqImt3JiZcTohUDJ3t3pe19Se38xuMLPfm9lhM3ugmXUBaC1r9Nx+M5sg6ZCkRZL6Jb0t6S53/11iGfb8QMlasee/UtJhdz/i7qcl/VTS4ibWB6CFmgn/pZL+NOJxf/bc3zGzLjPbY2Z7mtgWgII184XfaIcW5xzWu3u3pG6Jw36gnTSz5++XNHvE4y9JOtZcOwBapZnwvy1pvpl92cwmSfqGpJ3FtAWgbA0f9rv7GTO7V9IvJU2QtMndDxTWGYBSNTzU19DG+MwPlK4lJ/kAGL8IPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCKrhKbolycyOSvpQ0meSzrh7RxFNoX1cdtllyXpnZ2eLOhn7tpctW1azZpaeyLbZ2au3bNmSrN99991Nrb8ITYU/82/ufqqA9QBoIQ77gaCaDb9L2mVme82sq4iGALRGs4f9V7v7MTObIelXZva/7t478gXZHwX+MABtpqk9v7sfy25PSHpO0pWjvKbb3Tv4MhBoLw2H38wuMbPJw/cldUp6t6jGAJSrmcP+mZKey4ZMJkrqcfcXC+kKQOms2fHMMW3MrHUbC2TBggU1a6tWrUoue/nllyfrU6ZMSdbnz5+frEfV39+frOedP9EMd0+fxJBhqA8IivADQRF+ICjCDwRF+IGgCD8QVBFX9aFkS5cuTdZ7enpq1iZNmlR0O+eFTz/9NFnPGyLN88orrzS1fCuw5weCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnbwMrVqxI1jds2JCsT5gwoch22kZvb2+y/t577yXra9asqVkbGBhILnv69Olk/XzAnh8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmKcvwXyxvEfe+yxZL2Zcfy869aPHDmSrO/fvz9ZX79+fbJ+880316zt3r07uewbb7yRrJ86xeTQzWDPDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB5U7RbWabJH1d0gl3X5A9N03SNklzJB2VdIe7/yV3Y+fpFN15v6u/bdu2ZL3M6/HzrnmfO3duadtGNYqcovvHkm4467kHJL3s7vMlvZw9BjCO5Ibf3XslfXDW04slbc7ub5a0pOC+AJSs0c/8M939uCRltzOKawlAK5R+br+ZdUnqKns7AMam0T3/gJnNkqTs9kStF7p7t7t3uHtHg9sCUIJGw79T0vClaisk7SimHQCtkht+M3tK0m8k/aOZ9ZvZSkmPSlpkZn+QtCh7DGAcyR3nL3Rj43icf8qUKTVrPT09yWWvv/76otup2+DgYLK+devWZH3jxo3J+uuvvz7mnlCuIsf5AZyHCD8QFOEHgiL8QFCEHwiK8ANBMdRXp3Xr1tWs3XfffS3s5Fzvv/9+zdr06dObWvfJkyeT9ZUrVybrzz//fFPbx9gx1AcgifADQRF+ICjCDwRF+IGgCD8QFOEHgmKcv04LFy6sWevr62tq3XlTTT/++OPJemosffny5cllV61alazn+eijj5L1W2+9tWbtpZdeamrbGB3j/ACSCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMb56zRxYu2ZzaZNm5Zc9qqrrkrWBwYGkvXdu3cn6ykzZqSnUezu7k7WOzs7k/ULL7wwWU+dB3DnnXcml33hhReSdYyOcX4ASYQfCIrwA0ERfiAowg8ERfiBoAg/EFTuOL+ZbZL0dUkn3H1B9txDkr4lafhH3Ve7+y9yNzaOx/mjuv/++5P1hx9+OFlPnQfw2muvJZe96aabkvVPPvkkWY+qyHH+H0u6YZTn/9vdr8j+5QYfQHvJDb+790r6oAW9AGihZj7z32tmvzWzTWY2tbCOALREo+H/kaSvSLpC0nFJ36/1QjPrMrM9ZranwW0BKEFD4Xf3AXf/zN0HJW2QdGXitd3u3uHuHY02CaB4DYXfzGaNeLhU0rvFtAOgVWpfp5oxs6ckXSvpC2bWL+l7kq41syskuaSjku4psUcAJeB6fjTlnnvSf/efeOKJhtedd73/M8880/C6z2dczw8gifADQRF+ICjCDwRF+IGgCD8QFEN9aMrUqenLOlI/Oz5v3rzksmvXrk3Wt27dmqwfOnQoWT9fMdQHIInwA0ERfiAowg8ERfiBoAg/EBThB4JinD/z9NNPJ+uDg4M1a08++WRy2RdffLGhnsaDCy5I7z9SP+29evXq5LIff/xxsn7dddcl62+99Vayfr5inB9AEuEHgiL8QFCEHwiK8ANBEX4gKMIPBJX7u/1RTJ48OVlftGhRzVpvb2/R7YwbM2bMSNbzxvJTLr744mR92bJlyXrUcf56secHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaByx/nNbLakLZK+KGlQUre7/9DMpknaJmmOpKOS7nD3v5TXarm2b9+erKfG+R988MHksq+++mqyfuDAgWS9StOnT0/W169fX9q29+7dm6yvW7eutG1HUM+e/4ykVe5+uaR/lvRtM/uqpAckvezu8yW9nD0GME7kht/dj7t7X3b/Q0kHJV0qabGkzdnLNktaUlaTAIo3ps/8ZjZH0tckvSlpprsfl4b+QEhKn+cJoK3UfW6/mX1e0nZJ33H3v5rV9TNhMrMuSV2NtQegLHXt+c3scxoK/k/c/dns6QEzm5XVZ0k6Mdqy7t7t7h3u3lFEwwCKkRt+G9rFb5R00N1/MKK0U9KK7P4KSTuKbw9AWeo57L9a0jclvWNm+7LnVkt6VNLPzGylpD9Kur2cFlvjzJkzyfrp06dr1vIua92xI/13ccmS9Helhw8fTtZTvU+c2NxV2xs3bkzWb7nllobXnfo5dEl65JFHkvWTJ082vG3UEX53f11SrQ/46R9OB9C2OMMPCIrwA0ERfiAowg8ERfiBoAg/EBRTdNdp4cKFNWt9fX0t7ORcPT09NWvLly9vYSdjs2vXrmT9xhtvbFEn5xem6AaQRPiBoAg/EBThB4Ii/EBQhB8IivADQTHOX6eLLrqoZm3NmjXJZW+/Pf1TB3Pnzm2op/Hg2LFjNWt5v2OQ99PdGB3j/ACSCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMb5W2DevHnJ+jXXXJOs33bbbcl6Z2fnmHuq1/79+5P1vCm69+3bV7PGOH45GOcHkET4gaAIPxAU4QeCIvxAUIQfCIrwA0HljvOb2WxJWyR9UdKgpG53/6GZPSTpW5KGJ0lf7e6/yFlXyHF+oJXqHeevJ/yzJM1y9z4zmyxpr6Qlku6Q9Dd3X1dvU4QfKF+94Z9Yx4qOSzqe3f/QzA5KurS59gBUbUyf+c1sjqSvSXoze+peM/utmW0ys6k1lukysz1mtqepTgEUqu5z+83s85Jek7TW3Z81s5mSTklySf+poY8G/56zDg77gZIV9plfkszsc5J+LumX7v6DUepzJP3c3RfkrIfwAyUr7MIeMzNJGyUdHBn87IvAYUslvTvWJgFUp55v+/9F0q8lvaOhoT5JWi3pLklXaOiw/6ike7IvB1PrYs8PlKzQw/6iEH6gfFzPDyCJ8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTuD3gW7JSk90Y8/kL2XDtq197atS+J3hpVZG+X1fvCll7Pf87Gzfa4e0dlDSS0a2/t2pdEb42qqjcO+4GgCD8QVNXh7654+ynt2lu79iXRW6Mq6a3Sz/wAqlP1nh9ARSoJv5ndYGa/N7PDZvZAFT3UYmZHzewdM9tX9RRj2TRoJ8zs3RHPTTOzX5nZH7LbUadJq6i3h8zsz9l7t8/Mbqqot9lm9oqZHTSzA2b2H9nzlb53ib4qed9afthvZhMkHZK0SFK/pLcl3eXuv2tpIzWY2VFJHe5e+Ziwmf2rpL9J2jI8G5KZ/ZekD9z90ewP51R3/26b9PaQxjhzc0m91ZpZ+m5V+N4VOeN1EarY818p6bC7H3H305J+KmlxBX20PXfvlfTBWU8vlrQ5u79ZQ/95Wq5Gb23B3Y+7e192/0NJwzNLV/reJfqqRBXhv1TSn0Y87ld7TfntknaZ2V4z66q6mVHMHJ4ZKbudUXE/Z8udubmVzppZum3eu0ZmvC5aFeEfbTaRdhpyuNrd/0nSjZK+nR3eoj4/kvQVDU3jdlzS96tsJptZeruk77j7X6vsZaRR+qrkfasi/P2SZo94/CVJxyroY1Tufiy7PSHpOQ19TGknA8OTpGa3Jyru5/+5+4C7f+bug5I2qML3LptZerukn7j7s9nTlb93o/VV1ftWRfjfljTfzL5sZpMkfUPSzgr6OIeZXZJ9ESMzu0RSp9pv9uGdklZk91dI2lFhL3+nXWZurjWztCp+79ptxutKTvLJhjIekzRB0iZ3X9vyJkZhZnM1tLeXhq547KmyNzN7StK1Grrqa0DS9yT9j6SfSfoHSX+UdLu7t/yLtxq9XasxztxcUm+1ZpZ+UxW+d0XOeF1IP5zhB8TEGX5AUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4L6P/kiXybqiq8OAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import library\n",
    "import matplotlib.pyplot as plt\n",
    "# We can check the dataloader\n",
    "_, (example_datas, labels) = next(enumerate(test_loader))\n",
    "sample = example_datas[0][0]\n",
    "# show the data\n",
    "plt.imshow(sample, cmap='gray', interpolation='none')\n",
    "print(\"Label: \"+ str(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we can start to build our CNN model\n",
    "## We first import the pytorch nn module and optimizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "## Then define the model class\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        #input channel 1, output channel 10\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, stride=1)\n",
    "        #input channel 10, output channel 20\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, stride=1)\n",
    "        #dropout layer\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        #fully connected layer\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_drop(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model and optimizer\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "device = \"cpu\"\n",
    "model = CNN().to(device) #using cpu here\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "##define train function\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10000):\n",
    "    model.train()\n",
    "    tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "    counter = 0\n",
    "    for batch_idx, (data, target) in enumerate(tk0):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        counter += 1\n",
    "        tk0.set_postfix(loss=(loss.item()*data.size(0) / (counter * train_loader.batch_size)))\n",
    "##define test function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79436250f7954b8ba51eb561bbf2e893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0784, Accuracy: 9761/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ff6d53ac824e969465426934a379e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0790, Accuracy: 9765/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f23ff624ad0457082b17b8d1e21d461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0792, Accuracy: 9753/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 3\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 24, 24]             260\n",
      "            Conv2d-2             [-1, 20, 8, 8]           5,020\n",
      "         Dropout2d-3             [-1, 20, 8, 8]               0\n",
      "            Linear-4                   [-1, 50]          16,050\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 21,840\n",
      "Trainable params: 21,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.15\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
